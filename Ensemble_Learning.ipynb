{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "### Content\n",
    "- [Ensemble Learning](#ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ensemble'></a> \n",
    "### Ensemble Learning and Random Forests\n",
    "> Ensemble learning represents the usage of multiple predictors that make a prediction. The final class/value will be determined by a majority vote (hard voting classifier).\n",
    "\n",
    "There are different Ensemble methods:\n",
    "- bagging\n",
    "- boosting\n",
    "- stacking\n",
    "\n",
    "Hard Voting Classifier\n",
    "- majority vote from the used classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.datasets import make_moons, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset to showcase code\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifiers\n",
    "log_clf = LogisticRegression(solver='lbfgs', random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svc_clf = SVC(gamma='scale', random_state=24)\n",
    "\n",
    "# with voting we can set hard (majority) or soft (weight) \n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), \n",
    "                                          ('rf', rnd_clf), \n",
    "                                          ('svc', svc_clf)],\n",
    "                              voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=42,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     crit...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=42, verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=False, random_state=24,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the hard voting classifier\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "# get the accuracy for each clf individual and combined score\n",
    "for clf in (log_clf, rnd_clf, svc_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the Ensamble performed better than each algorithm by itself.\n",
    "\n",
    "#### Soft Voting\n",
    "based on the probability of each predicted value. To use soft voting, all estimators have to be able to estimate the class probabilities (predict_proba())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifiers\n",
    "# need to change SVC, since we want to predict the probabilites for soft voting\n",
    "log_clf = LogisticRegression(solver='lbfgs', random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svc_clf = SVC(gamma='scale', probability=True ,random_state=24)\n",
    "\n",
    "# with voting we can set hard (majority) or soft (weight) \n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), \n",
    "                                          ('rf', rnd_clf), \n",
    "                                          ('svc', svc_clf)],\n",
    "                              voting='soft', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=42,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     crit...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=42, verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=True, random_state=24,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the soft voting classifier\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "# get the accuracy for each clf individual and combined score\n",
    "for clf in (log_clf, rnd_clf, svc_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting\n",
    "> Use the same algorithm for every predictor and train them on different random subsets of the training set.\n",
    "- Bagging: sampling with replacement (bootstrap=True)\n",
    "- Pasting: sampling without replacement (bootstrap=False)\n",
    "- uses the statistical mode (most frequent prediction) for classifications and mean for regressions to make a prediction on unseen data \n",
    "- each Bagging and Pasting can be used with sklearn for regression and classification\n",
    "- Bagging often results in better models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        presort='deprecated',\n",
       "                                                        random_state=42,\n",
       "                                                        splitter='best'),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=100, n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "                  random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "                           n_estimators=500,\n",
    "                           max_samples=100,\n",
    "                           bootstrap=True,\n",
    "                           random_state=42,\n",
    "                           n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856\n"
     ]
    }
   ],
   "source": [
    "# Result of a normal DecisionTree\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_preds = tree_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, tree_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag evaluation\n",
    "> With Bagging, some instances may be sampled several times for any given predictor, while others my nor be sampled at all. OOB uses these instances without the need for a separate validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set oob_score=True in BaggingClassifier\n",
    "bag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "                           n_estimators=500,\n",
    "                           bootstrap=True,\n",
    "                           oob_score=True,\n",
    "                           random_state=42)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912\n"
     ]
    }
   ],
   "source": [
    "# check the accuracy prediction from oob with real prediction \n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32352941, 0.67647059],\n",
       "       [0.35625   , 0.64375   ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns the class probabilities for each instance\n",
    "bag_clf.oob_decision_function_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Patches and Random Subspaces\n",
    "> BaggingClassifier supports sampling features, allowing the predictor to be trained on a random subset of the input features.\n",
    "- useful when dealing with high dimensional space (images)\n",
    "\n",
    "> sampling features results in even more predictor diversity, trading a bit more bias for a lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests and Extra Trees\n",
    "- A Random Forest is an ensemble of Decision Trees, which introduces more randomness when growing trees. Rather than searching for the best feature when splitting it searches for the best feature in a random feature set. -> more diversity / higher bias for lower variance -> generally the better model\n",
    "- Extra-Tress introduce even more randomness for higher diversity and bias-variance trade-off.\n",
    "    - random thresholds when searching for best possible thresholds \n",
    "    \n",
    "> Hard to tell which algorithm performs better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "> Random Forests make it easy to measure the relative importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10018256738063595\n",
      "sepal width (cm) 0.02339091270538694\n",
      "petal length (cm) 0.44045983501268\n",
      "petal width (cm) 0.43596668490129703\n"
     ]
    }
   ],
   "source": [
    "# check the feature importance of the iris dataset\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So they are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMME.R retuns class probabilities (predict_proba)\n",
    "ada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "                            n_estimators=200,\n",
    "                            algorithm='SAMME.R',\n",
    "                             learning_rate=0.5,\n",
    "                             random_state=42\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.896\n"
     ]
    }
   ],
   "source": [
    "y_pred = ada_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "> works like AdaBoost but fits the new predictor to the residual errors made by the previous predictor.\n",
    "\n",
    "regularization technique: shrinkage\n",
    "- set a low value for the learning rate to get predictions that will usually generalize better.\n",
    "\n",
    "##### Find optimal number of trees\n",
    "- Early stopping with the staged_predict() method \n",
    "\n",
    "##### Stochastic Gradient Boosting\n",
    "> using Gradient Boosting with the hyperparameter subsample to train each tree on a randomly selected subsample of the training data.\n",
    "\n",
    "- trades higher bias for lower variance \n",
    "- speeds up training considerably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=56, random_state=42)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=49)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_test, y_pred) for y_pred in gbrt.staged_predict(X_test)]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_error = np.min(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the gbrt_best is trained with 56 trees\n",
    "bst_n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is also possible to implement early stopping by actually stopping training early \n",
    "- we have to set `warm_start` = True, which reuses the solution of the previous call to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "# implement early stopping, when the validation error does not improve for five iterations in a row\n",
    "\n",
    "# base variables\n",
    "# set first error to inf so that we can continue with every error, regardless of size\n",
    "min_val_error = float('inf')\n",
    "error_going_up = 0\n",
    "\n",
    "# for loop\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_test)\n",
    "    val_error = mean_squared_error(y_test, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up +=1\n",
    "        if error_going_up == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "print(gbrt.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation MSE: 0.002712853325235463\n"
     ]
    }
   ],
   "source": [
    "print('Minimum validation MSE:', min_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "> XGBoost is an optimized implementation of Gradient Boosting. It aims to be extremely fast, scalable, and portable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 0.00400040950714611\n"
     ]
    }
   ],
   "source": [
    "xgb_reg = xgboost.XGBRegressor(random_state=42)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "val_error = mean_squared_error(y_test, y_pred)\n",
    "print(\"Validation MSE:\", val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It automatically takes care of early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.22834\n",
      "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-rmse:0.16224\n",
      "[2]\tvalidation_0-rmse:0.11843\n",
      "[3]\tvalidation_0-rmse:0.08760\n",
      "[4]\tvalidation_0-rmse:0.06848\n",
      "[5]\tvalidation_0-rmse:0.05709\n",
      "[6]\tvalidation_0-rmse:0.05297\n",
      "[7]\tvalidation_0-rmse:0.05129\n",
      "[8]\tvalidation_0-rmse:0.05155\n",
      "[9]\tvalidation_0-rmse:0.05211\n",
      "Stopping. Best iteration:\n",
      "[7]\tvalidation_0-rmse:0.05129\n",
      "\n",
      "Validation MSE: 0.0026308690413069744\n"
     ]
    }
   ],
   "source": [
    "xgb_reg.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "val_error = mean_squared_error(y_test, y_pred)\n",
    "print(\"Validation MSE:\", val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "> Instead of using trivial functions (hard voting) to aggregate the predictions of all predictors in an ensemble it uses a model to perform the aggregation.\n",
    "> The final predictor is called blender or meta learner \n",
    "\n",
    "- scikit learn does not support stacking, but it is not too hard to implement or one can use DESlib\n",
    "\n",
    "#### Implementation\n",
    "1. split training set into two subsets \n",
    "2. use the first to train the predictors in the first layer\n",
    "3. these predictors are used to make predictions on the second set\n",
    "4. create a new training set with the predicted values as input features \n",
    "5. the blender is trained on this set, so it learns to predict the target value, based on the first layer's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
