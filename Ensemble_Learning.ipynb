{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "> Ensemble learning represents the usage of multiple predictors that make a prediction.\n",
    "> Ensembling is about combining a collection of models to get a more performant model or help to address issues of overfitting by reducing the models variance \n",
    "\n",
    "- models used in ensemble learning are ofter called <b>weak learners</b>\n",
    "\n",
    "There are different Ensemble methods:\n",
    "- [Averaging](#avg)\n",
    "- [Weighted Averaging](#wavg)\n",
    "- [Hard or Max Voting](#hard)\n",
    "- [Soft Voting](#soft)\n",
    "- [Bagging and Pasting](#bag)\n",
    "- [Random Patches and Random Subspaces](#random)\n",
    "- [Random Forest and Extra Trees](#rf)\n",
    "- [Boosting](#boosting)\n",
    "- [Stacking](#stacking)\n",
    "\n",
    "\n",
    "### Advantages and disadvantages of Ensemble Learning\n",
    "| Advantages | Disadvantages |\n",
    "|------------|---------------|\n",
    "| higher predictive power| less interpretable/hard to sell in business to get insights |\n",
    "| useful when there is linear and non-linear data |  hard to learn, wrong selections can lower predictive power |\n",
    "| bias/variance can be reduced as well as under/overfitting | expensive both in time and space |\n",
    "| less noisy and more stable |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.datasets import make_moons, load_iris, load_diabetes, load_boston\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV, Lasso, ElasticNet, Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.1\n"
     ]
    }
   ],
   "source": [
    "# check version, stacking is supported from 0.22\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='avg'></a>\n",
    "### Averaging\n",
    "calculate the mean of the predictions from each predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn Mean Absolute Error: 3.6215748031496067\n",
      "lasso Mean Absolute Error: 3.424610243097111\n",
      "ridge Mean Absolute Error: 3.0503751260061707\n",
      "rf Mean Absolute Error: 2.1682125984251974\n",
      "AVERAGING Mean Absolute Error: 2.5746191369884746\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "X, y = load_boston(return_X_y=True)\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# define models\n",
    "knn = KNeighborsRegressor()\n",
    "lasso = Lasso()\n",
    "ridge = Ridge()\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# fit models\n",
    "knn.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "knn_pred = knn.predict(X_test)\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "# Averaging\n",
    "avg_pred = np.mean([knn_pred, lasso_pred, ridge_pred, rf_pred], axis=0)\n",
    "\n",
    "# Evaluation\n",
    "print('knn Mean Absolute Error:', mean_absolute_error(y_test, knn_pred))\n",
    "print('lasso Mean Absolute Error:', mean_absolute_error(y_test, lasso_pred))\n",
    "print('ridge Mean Absolute Error:', mean_absolute_error(y_test, ridge_pred))\n",
    "print('rf Mean Absolute Error:', mean_absolute_error(y_test, rf_pred))\n",
    "print('AVERAGING Mean Absolute Error:', mean_absolute_error(y_test, avg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the averaging method didn't result in a lower error in this case but we can be sure that the prediction will generalize better since it is based one three models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wavg'></a>\n",
    "### Weighted Averaging\n",
    "we saw that simple averaging doesn't always result into lower error rates. Therefore we can assign weights to each prediction given the most weight to to ones that performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn Mean Absolute Error: 3.6215748031496067\n",
      "lasso Mean Absolute Error: 3.424610243097111\n",
      "ridge Mean Absolute Error: 3.0503751260061707\n",
      "rf Mean Absolute Error: 2.1682125984251974\n",
      "AVERAGING Mean Absolute Error: 2.5746191369884746\n",
      "Weighted AVERAGING Mean Absolute Error: 2.395877374561435\n"
     ]
    }
   ],
   "source": [
    "weighted_pred = knn_pred*0.1 + lasso_pred*0.2 + ridge_pred*0.3 + rf_pred*0.4\n",
    "\n",
    "print('knn Mean Absolute Error:', mean_absolute_error(y_test, knn_pred))\n",
    "print('lasso Mean Absolute Error:', mean_absolute_error(y_test, lasso_pred))\n",
    "print('ridge Mean Absolute Error:', mean_absolute_error(y_test, ridge_pred))\n",
    "print('rf Mean Absolute Error:', mean_absolute_error(y_test, rf_pred))\n",
    "print('AVERAGING Mean Absolute Error:', mean_absolute_error(y_test, avg_pred))\n",
    "print('Weighted AVERAGING Mean Absolute Error:', mean_absolute_error(y_test, weighted_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> using the weighted averaging results in lower errors than simple averaging but in this case it still performed slightly worse than the best single predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hard'></a>\n",
    "### Hard Voting or Max Voting\n",
    "- is very much the same as averaging but for classification tasks  \n",
    "- the vote with the majority will used as the prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset to showcase code\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "# create classifiers\n",
    "log_clf = LogisticRegression(solver='lbfgs', random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svc_clf = SVC(gamma='scale', random_state=24)\n",
    "\n",
    "# with voting we can set hard (majority) or soft (weight) \n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), \n",
    "                                          ('rf', rnd_clf), \n",
    "                                          ('svc', svc_clf)],\n",
    "                              voting='hard')\n",
    "\n",
    "# fit the hard voting classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy for each clf individual and combined score\n",
    "for clf in (log_clf, rnd_clf, svc_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As expected the max voting classifier performed better than each algorithm by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='soft'></a>\n",
    "### Soft Voting\n",
    "based on the probability of each predicted value. To use soft voting, all estimators have to be able to estimate the class probabilities (predict_proba())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "# create classifiers\n",
    "# need to change SVC, since we want to predict the probabilites for soft voting\n",
    "log_clf = LogisticRegression(solver='lbfgs', random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svc_clf = SVC(gamma='scale', probability=True ,random_state=24)\n",
    "\n",
    "# with voting we can set hard (majority) or soft (weight) \n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), \n",
    "                                          ('rf', rnd_clf), \n",
    "                                          ('svc', svc_clf)],\n",
    "                              voting='soft')\n",
    "\n",
    "# fit the soft voting classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# get the accuracy for each clf individual and combined score\n",
    "for clf in (log_clf, rnd_clf, svc_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> slightly better results than hard voting, because there is more information in probabilities than just the majority vote (soft voting takes the certainty of a classifier into account)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bag'></a>\n",
    "### Bagging and Pasting\n",
    "Use the same algorithm for every predictor and train them on different random subsets of the training set.\n",
    "- Parallel method: no dependency between base learners\n",
    "- Bagging: sampling with replacement (bootstrap=True)\n",
    "- Pasting: sampling without replacement (bootstrap=False)\n",
    "- uses the statistical mode (most frequent prediction) for classifications and mean for regressions to make a prediction on unseen data \n",
    "- each Bagging and Pasting can be used with sklearn for regression and classification\n",
    "- every model is trained on a different subset of data and all the results are combined, so the final model is less overfitted and variance is reduces\n",
    "- effective for models which have high variance like classification and regression trees\n",
    "\n",
    "Bagging out of the Box: `RandomForest` and `ExtraTreesREgressor`\n",
    "\n",
    "Transform every algorithm into Bagging: `BaggingClassifier` or `BaggingRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier : 0.904\n",
      "PastingClassifier : 0.92\n",
      "DesisionTree : 0.856\n"
     ]
    }
   ],
   "source": [
    "# define bagging classifier (bootstrap=True)\n",
    "bag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "                            n_estimators=500,\n",
    "                            max_samples=100,\n",
    "                            bootstrap=True,\n",
    "                            random_state=42,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "# define pasting classifier (bootsrtap=False)\n",
    "past_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "                            n_estimators=500,\n",
    "                            max_samples=100,\n",
    "                            bootstrap=False,\n",
    "                            random_state=42,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "# normal classifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# fit\n",
    "bag_clf.fit(X_train, y_train)\n",
    "past_clf.fit(X_train, y_train)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "bag_pred = bag_clf.predict(X_test)\n",
    "past_pred = past_clf.predict(X_test)\n",
    "tree_preds = tree_clf.predict(X_test)\n",
    "\n",
    "# Result\n",
    "print('BaggingClassifier :', accuracy_score(y_test, bag_pred))\n",
    "print('PastingClassifier :', accuracy_score(y_test, past_pred))\n",
    "print('DesisionTree :', accuracy_score(y_test, tree_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> as expected Bagging and Pasting performed better than the normal classifier. Bagging is usually preferred because it adds extra diversity so that the ensemble's variance is reduced, at the cost of a little more bias (as we see in the results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag evaluation\n",
    "With Bagging, some instances may be sampled several times for any given predictor, while others my nor be sampled at all. OOB uses these instances without the need for a separate validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set oob_score=True in BaggingClassifier\n",
    "bag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "                            n_estimators=500,\n",
    "                            bootstrap=True,\n",
    "                            oob_score=True,\n",
    "                            random_state=42)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912\n"
     ]
    }
   ],
   "source": [
    "# check the accuracy prediction from oob with real prediction \n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32352941, 0.67647059],\n",
       "       [0.35625   , 0.64375   ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns the class probabilities for each instance\n",
    "bag_clf.oob_decision_function_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='random'></a>\n",
    "### Random Patches and Random Subspaces\n",
    "Till now we sampled on the row basis. <b>Random Patches</b> combines sampling training instances and features, which is particularly useful for high-dimensional data. <b>Random Subspaces</b> keeps all training instances but samples input features.\n",
    "\n",
    "> sampling features results in even more predictor diversity, trading a bit more bias for a lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rf'></a>\n",
    "### Random Forests and Extra Trees\n",
    "`RandomForest` is an ensemble of Decision Trees, which introduces more randomness when growing trees. Rather than searching for the best feature when splitting it searches for the best feature in a random feature set. -> more diversity / higher bias for lower variance -> generally the better model\n",
    "\n",
    "`ExtraTress` introduce even more randomness for higher diversity and bias-variance trade-off.\n",
    "    - random thresholds when searching for best possible thresholds \n",
    "    \n",
    "> Hard to tell which algorithm performs better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance\n",
    "> Random Forests make it easy to measure the relative importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10018256738063595\n",
      "sepal width (cm) 0.02339091270538694\n",
      "petal length (cm) 0.44045983501268\n",
      "petal width (cm) 0.43596668490129703\n"
     ]
    }
   ],
   "source": [
    "# check the feature importance of the iris dataset\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So they are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boosting'></a>\n",
    "### Boosting\n",
    "The general idea of most boosting methods is to train predictors <b>sequentially</b>, each trying to correct its predecessor. The goal is to convert a weak learner to a strong learner. There are two main boosting methods: AdaBoost and Gradient Boosting.\n",
    "\n",
    "- iterative method that adjusts the weights of an observation based on the previous classification \n",
    "- Goal: decrease the base error (can overfitt)\n",
    "\n",
    "#### AdaBoost\n",
    "- pays more attention to the training instances that the predecessor underfitted\n",
    "\n",
    "#### Gradient Boosting\n",
    "- works like AdaBoost but fits the new predictor to the residual errors made by the previous predictor.\n",
    "\n",
    "regularization technique: shrinkage\n",
    "- set a low value for the learning rate to get predictions that will usually generalize better.\n",
    "\n",
    "##### Find optimal number of trees\n",
    "- Early stopping with the staged_predict() method \n",
    "\n",
    "##### Stochastic Gradient Boosting\n",
    "> using Gradient Boosting with the hyperparameter subsample to train each tree on a randomly selected subsample of the training data.\n",
    "\n",
    "- trades higher bias for lower variance \n",
    "- speeds up training considerably\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier : Accuracy 0.904\n",
      "GradientBoostingClassifier : Accuracy 0.888\n",
      "XGBClassifier : Accuracy 0.872\n"
     ]
    }
   ],
   "source": [
    "# define boosting algos\n",
    "ada_clf = AdaBoostClassifier()\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "xgb_clf = XGBClassifier()\n",
    "\n",
    "classifiers = [ada_clf, gb_clf, xgb_clf]\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, ': Accuracy', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> it is quite surprising to see that extreme gradient boosting performed worse then the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping\n",
    "It is also possible to implement early stopping by actually stopping training early \n",
    "- we have to set `warm_start` = True, which reuses the solution of the previous call to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "X, y = load_boston(return_X_y=True)\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "# implement early stopping, when the validation error does not improve for five iterations in a row\n",
    "\n",
    "# base variables\n",
    "# set first error to inf so that we can continue with every error, regardless of size\n",
    "min_val_error = float('inf')\n",
    "error_going_up = 0\n",
    "\n",
    "# for loop\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_test)\n",
    "    val_error = mean_squared_error(y_test, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up +=1\n",
    "        if error_going_up == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "print(gbrt.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation MSE: 12.979089325435462\n"
     ]
    }
   ],
   "source": [
    "print('Minimum validation MSE:', min_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "> XGBoost is an optimized implementation of Gradient Boosting. It aims to be extremely fast, scalable, and portable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 10.061070591701837\n"
     ]
    }
   ],
   "source": [
    "xgb_reg = XGBRegressor(random_state=42)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "val_error = mean_squared_error(y_test, y_pred)\n",
    "print(\"Validation MSE:\", val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It automatically takes care of early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:16.17967\n",
      "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-rmse:11.82363\n",
      "[2]\tvalidation_0-rmse:9.00198\n",
      "[3]\tvalidation_0-rmse:7.09986\n",
      "[4]\tvalidation_0-rmse:5.75756\n",
      "[5]\tvalidation_0-rmse:4.89611\n",
      "[6]\tvalidation_0-rmse:4.25794\n",
      "[7]\tvalidation_0-rmse:3.95111\n",
      "[8]\tvalidation_0-rmse:3.78119\n",
      "[9]\tvalidation_0-rmse:3.64725\n",
      "[10]\tvalidation_0-rmse:3.50068\n",
      "[11]\tvalidation_0-rmse:3.42377\n",
      "[12]\tvalidation_0-rmse:3.37779\n",
      "[13]\tvalidation_0-rmse:3.35327\n",
      "[14]\tvalidation_0-rmse:3.32273\n",
      "[15]\tvalidation_0-rmse:3.27602\n",
      "[16]\tvalidation_0-rmse:3.24714\n",
      "[17]\tvalidation_0-rmse:3.22577\n",
      "[18]\tvalidation_0-rmse:3.22214\n",
      "[19]\tvalidation_0-rmse:3.22225\n",
      "[20]\tvalidation_0-rmse:3.20872\n",
      "[21]\tvalidation_0-rmse:3.21014\n",
      "[22]\tvalidation_0-rmse:3.18628\n",
      "[23]\tvalidation_0-rmse:3.18931\n",
      "[24]\tvalidation_0-rmse:3.19223\n",
      "Stopping. Best iteration:\n",
      "[22]\tvalidation_0-rmse:3.18628\n",
      "\n",
      "Validation MSE: 10.152398456601889\n"
     ]
    }
   ],
   "source": [
    "xgb_reg.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "val_error = mean_squared_error(y_test, y_pred)\n",
    "print(\"Validation MSE:\", val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stacking'></a>\n",
    "### Stacking (stacked generalization)\n",
    "Instead of using trivial functions (hard voting) to aggregate the predictions of all predictors in an ensemble it uses a model to perform the aggregation. The final predictor is called blender or meta learner. The base learners are used to make a prediction which is then used as inputs in the meta leaner to make the final prediction\n",
    "\n",
    "- with version 0.22 of Scikit-learn stacking is now supported by scikit learn\n",
    "- before the update one had to use mlxtend or vecstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeCV R2 score: 0.68\n",
      "Lasso R2 score: 0.65\n",
      "ElasticNet R2 score: 0.66\n",
      "RandomForestRegressor R2 score: 0.84\n",
      "GradientBoostingRegressor R2 score: 0.87\n",
      "XGBRegressor R2 score: 0.86\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeCV()\n",
    "lasso = Lasso()\n",
    "elnet = ElasticNet()\n",
    "rf = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "models = [ridge, lasso, elnet, rf, gb, xgb]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    print(model.__class__.__name__, 'R2 score: {:.2f}'.format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.83\n"
     ]
    }
   ],
   "source": [
    "# build stacking model\n",
    "estimators = [\n",
    "    ('ridge', RidgeCV()),\n",
    "    ('lasso', Lasso()),\n",
    "    ('rf', RandomForestRegressor(n_estimators=10, random_state=42))]\n",
    "\n",
    "reg = StackingRegressor(estimators=estimators, final_estimator=GradientBoostingRegressor(random_state=42))\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "print('R2 score: {:.2f}'.format(reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple stacking layers\n",
    "- assigning the `final_estimator` to a `StackingRegressor` or `Classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.88\n"
     ]
    }
   ],
   "source": [
    "final_layer = StackingRegressor(estimators = \n",
    "                                [('rf', RandomForestRegressor(n_estimators=10, random_state=42)),\n",
    "                                 ('gb', GradientBoostingRegressor(random_state=42))], \n",
    "                                final_estimator=RidgeCV())\n",
    "\n",
    "multi_layer_reg = StackingRegressor(estimators = \n",
    "                                [('lr', RidgeCV()),\n",
    "                                 ('xgb', XGBRegressor()),\n",
    "                                 ('svr', SVR(C=1, gamma=1e-6, kernel='rbf'))],\n",
    "                                final_estimator=final_layer)\n",
    "                                    \n",
    "multi_layer_reg.fit(X_train, y_train)\n",
    "print('R2 score: {:.2f}'.format(multi_layer_reg.score(X_test, y_test)))                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- Hands on Machine Learning, second edition\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html#stacking\n",
    "- https://towardsdatascience.com/stacking-made-easy-with-sklearn-e27a0793c92b\n",
    "- https://towardsdatascience.com/ensemble-learning-techniques-6346db0c6ef8\n",
    "- https://towardsdatascience.com/advanced-ensemble-learning-techniques-bf755e38cbfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
