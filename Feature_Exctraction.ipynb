{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for Data Preparation\n",
    "\n",
    "https://machinelearningmastery.com/feature-extraction-on-tabular-data/\n",
    "\n",
    "\n",
    "##### Common Data Preparation Approach (slow, expensive, requires experience)\n",
    "1. study dataset\n",
    "2. review expectations of the ML algo\n",
    "3. carefully chose the appropriate transformations\n",
    "\n",
    "##### Alternative Data Preparation Approach\n",
    "1. apply a suite of common and commonly useful techniques\n",
    "2. aggregate all features together and create one large dataset\n",
    "\n",
    "### Performance of the different Methods\n",
    "Tested with a simple LogisticRegression with a 'liblinear' solver (small dataset)\n",
    "- Baseline: Accuracy of 0.953\n",
    "- Feature Extraction: Accuracy of 0.968\n",
    "- Feature Extraction + Selection: Accuracy of 0.989\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer, KBinsDiscretizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/wine.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for this is the wine dataset which has information about the chemical analysis of wine, which are from three different cultivars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13) (178,)\n"
     ]
    }
   ],
   "source": [
    "data = df.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model Performance\n",
    "test the base performance with raw input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimal preperartion to meet the expected input from sklearn\n",
    "X = X.astype('float')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "# for small datasets liblinear is a good choice\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# define cross validation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate model\n",
    "score = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.953 (0.048)\n"
     ]
    }
   ],
   "source": [
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(score), np.std(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction Approach to Data Preparation\n",
    "test if we can improve performance with a suit of common and commonly used data preparation techniques:\n",
    "- Scaling\n",
    "    - MinMaxScaler\n",
    "    - StandarScaler\n",
    "    - RobustScaler\n",
    "- Distribution transformers\n",
    "    - QuantileTransformer\n",
    "    - KBinsDiscretizer\n",
    "- Dimensionality Reduction\n",
    "    - PCA\n",
    "    - TruncatedSVD\n",
    "    \n",
    "With the feature union class we can define a list of transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "data = df.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# minimal preperartion to meet the expected input from sklearn\n",
    "X = X.astype('float')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers for the feature union\n",
    "transformers = list()\n",
    "transformers.append(('mms', MinMaxScaler()))\n",
    "transformers.append(('ss', StandardScaler()))\n",
    "transformers.append(('rs', RobustScaler()))\n",
    "transformers.append(('qt', QuantileTransformer(n_quantiles=100, output_distribution='normal')))\n",
    "transformers.append(('kbd', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')))\n",
    "transformers.append(('pca', PCA(n_components=7)))\n",
    "transformers.append(('svd', TruncatedSVD(n_components=7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mms', MinMaxScaler()),\n",
       " ('ss', StandardScaler()),\n",
       " ('rs', RobustScaler()),\n",
       " ('qt', QuantileTransformer(n_quantiles=100, output_distribution='normal')),\n",
       " ('kbd', KBinsDiscretizer(encode='ordinal', n_bins=10, strategy='uniform')),\n",
       " ('pca', PCA(n_components=7)),\n",
       " ('svd', TruncatedSVD(n_components=7))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crate feature union\n",
    "fu = FeatureUnion(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipline\n",
    "steps = list()\n",
    "steps.append(('fu', fu))\n",
    "steps.append(('m', model))\n",
    "pipeline = Pipeline(steps=steps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the cross-validation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.968 (0.037)\n"
     ]
    }
   ],
   "source": [
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Now add feature selection to the process.\n",
    "\n",
    "With Recursive Feature Elimination\n",
    "> RFE works by searching for a subset of features by starting with all features in the training dataset and successfully removing features until the desired number remains.\n",
    "- is is a wrapper-type feature selection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "data = df.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# minimal preperartion to meet the expected input from sklearn\n",
    "X = X.astype('float')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers for the feature union\n",
    "transformers = list()\n",
    "transformers.append(('mms', MinMaxScaler()))\n",
    "transformers.append(('ss', StandardScaler()))\n",
    "transformers.append(('rs', RobustScaler()))\n",
    "transformers.append(('qt', QuantileTransformer(n_quantiles=100, output_distribution='normal')))\n",
    "transformers.append(('kbd', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')))\n",
    "transformers.append(('pca', PCA(n_components=7)))\n",
    "transformers.append(('svd', TruncatedSVD(n_components=7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crate feature union\n",
    "fu = FeatureUnion(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the feature selection\n",
    "rfe = RFE(estimator=LogisticRegression(solver='liblinear'), n_features_to_select=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pipeline\n",
    "steps = list()\n",
    "steps.append(('fu', fu))\n",
    "steps.append(('rfe', rfe))\n",
    "steps.append(('m', model))\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the cross-validation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.989 (0.022)\n"
     ]
    }
   ],
   "source": [
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
